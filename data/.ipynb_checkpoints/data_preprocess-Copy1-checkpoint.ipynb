{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: amazon-book, core: 10\n",
      "before core filter:\t    846434, user(70679), item(24915)\n",
      "after core filter:\t    467791, user(18441), item(15717)\n",
      "before add neg, # of interaction: 467791\n",
      "splitting dataset ...\n",
      "add neg, # of interaction: 935582\n",
      "rating_file saved!\n"
     ]
    }
   ],
   "source": [
    "dataset = 'amazon-book'\n",
    "# dataset = 'yelp2018'\n",
    "# dataset = 'last-fm'\n",
    "core = 10\n",
    "\n",
    "print('dataset: %s, core: %d' % (dataset, core))\n",
    "rating_file = dataset + '/ratings_final'\n",
    "train_file = dataset + '/train'\n",
    "test_file = dataset + '/test'\n",
    "rating_arr = []\n",
    "with open(train_file + '.txt') as f:\n",
    "    lines = [line.split() for line in f]\n",
    "    for line in lines:\n",
    "        u = int(line[0])\n",
    "        items = line[1:]\n",
    "        rating_arr += [[u, int(i), 1] for i in items]\n",
    "\n",
    "with open(test_file + '.txt') as f:\n",
    "    lines = [line.split() for line in f]\n",
    "    for line in lines:\n",
    "        u = int(line[0])\n",
    "        items = line[1:]\n",
    "        rating_arr += [[u, int(i), 1] for i in items]\n",
    "rating_np = np.array(rating_arr, dtype=np.int64)\n",
    "\n",
    "    \n",
    "item_counter = {}\n",
    "user_counter = {}\n",
    "for u, i, _ in rating_np:\n",
    "    if i not in item_counter:\n",
    "        item_counter[i] = 1\n",
    "    else:\n",
    "        item_counter[i] += 1\n",
    "    \n",
    "    if u not in user_counter:\n",
    "        user_counter[u] = 1\n",
    "    else:\n",
    "        user_counter[u] += 1\n",
    "\n",
    "        \n",
    "n_user = len(set(rating_np[:, 0]))\n",
    "n_item = len(set(rating_np[:, 1]))\n",
    "print('before core filter:\\t%10d, user(%d), item(%d)' % (len(rating_np), n_user, n_item))\n",
    "core_rating_np = []\n",
    "all_rating_np = []\n",
    "user_history_dict = {}\n",
    "for u, i, _ in rating_np:\n",
    "    if user_counter[u] > core and item_counter[i] > core:\n",
    "        core_rating_np += [[u, i, 1]]\n",
    "        if u not in user_history_dict:\n",
    "            user_history_dict[u] = set()\n",
    "        user_history_dict[u].add(i)\n",
    "    all_rating_np += [[u, i, 1]]\n",
    "print('after core filter:\\t%10d' % (len(core_rating_np)), end='')\n",
    "rating_np = np.array(core_rating_np)\n",
    "\n",
    "n_user = len(set(rating_np[:, 0]))\n",
    "n_item = len(set(rating_np[:, 1]))\n",
    "item_set = set(rating_np[:, 1])\n",
    "print(', user(%d), item(%d)' % (n_user, n_item))\n",
    "\n",
    "all_rating_np = np.array(all_rating_np)\n",
    "all_item_set = set(all_rating_np[:, 1])\n",
    "\n",
    "\n",
    "print('before add neg, # of interaction: %d' % (len(rating_np)))\n",
    "neg_data = []\n",
    "for u, i_set in user_history_dict.items():\n",
    "    neg_i = list(all_item_set - i_set)\n",
    "    neg_i = random.sample(neg_i, min(len(neg_i), len(i_set)))\n",
    "    neg_data += [[u, i, 0] for i in neg_i]\n",
    "\n",
    "neg_data = np.array(neg_data)\n",
    "rating_np = np.append(rating_np, neg_data, axis=0)\n",
    "\n",
    "train_data, eval_data, test_data, user_history_dict = dataset_split(rating_np)\n",
    "print('add neg, # of interaction: %d' % (len(rating_np)))\n",
    "np.save(rating_file, rating_np)\n",
    "print('rating_file saved!')\n",
    "\n",
    "title = [['user', 'item', 'like']]\n",
    "def kk(d, t):\n",
    "    data = {\n",
    "        'user': d[:, 0],\n",
    "        'item': d[:, 1],\n",
    "        'like': d[:, 2]\n",
    "    }\n",
    "    df = DataFrame(data, columns=['user', 'item', 'like'])\n",
    "    df.to_csv(f'data_filter/{dataset}/{t}_pd.csv')\n",
    "\n",
    "kk(train_data, 'train')\n",
    "kk(test_data, 'test')\n",
    "kk(eval_data, 'eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(rating_np):\n",
    "    print('splitting dataset ...')\n",
    "\n",
    "    # train:eval:test = 6:2:2\n",
    "    eval_ratio = 0.2\n",
    "    test_ratio = 0.2\n",
    "    n_ratings = rating_np.shape[0]\n",
    "\n",
    "    eval_indices = np.random.choice(n_ratings, size=int(n_ratings * eval_ratio), replace=False)\n",
    "    left = set(range(n_ratings)) - set(eval_indices)\n",
    "    test_indices = np.random.choice(list(left), size=int(n_ratings * test_ratio), replace=False)\n",
    "    train_indices = list(left - set(test_indices))\n",
    "    # print(len(train_indices), len(eval_indices), len(test_indices))\n",
    "\n",
    "    # traverse training data, only keeping the users with positive ratings\n",
    "    user_history_dict = dict()\n",
    "    for i in train_indices:\n",
    "        user = rating_np[i][0]\n",
    "        item = rating_np[i][1]\n",
    "        rating = rating_np[i][2]\n",
    "\n",
    "        if rating == 1:\n",
    "            if user not in user_history_dict:\n",
    "                user_history_dict[user] = []\n",
    "            user_history_dict[user].append(item)\n",
    "\n",
    "    train_indices = [i for i in train_indices if rating_np[i][0] in user_history_dict]\n",
    "    eval_indices = [i for i in eval_indices if rating_np[i][0] in user_history_dict]\n",
    "    test_indices = [i for i in test_indices if rating_np[i][0] in user_history_dict]\n",
    "    # print(len(train_indices), len(eval_indices), len(test_indices))\n",
    "\n",
    "    train_data = rating_np[train_indices]\n",
    "    eval_data = rating_np[eval_indices]\n",
    "    test_data = rating_np[test_indices]\n",
    "\n",
    "    # print(train_data)\n",
    "    # input()\n",
    "\n",
    "    return train_data, eval_data, test_data, user_history_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
