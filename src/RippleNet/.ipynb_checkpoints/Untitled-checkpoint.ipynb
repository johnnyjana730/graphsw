{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rating(d):\n",
    "    print('reading rating file ...')\n",
    "\n",
    "    # reading rating file\n",
    "    rating_np = np.load('../data/' + d + '/ratings_final' + '.npy')\n",
    "\n",
    "    n_user = len(set(rating_np[:, 0]))\n",
    "    n_item = len(set(rating_np[:, 1]))\n",
    "    print('u %d, i %d' % (n_user, n_item))\n",
    "    user_history_dict = dict()\n",
    "    for t_i in range(rating_np.shape[0]):\n",
    "        user = rating_np[t_i][0]\n",
    "        item = rating_np[t_i][1]\n",
    "        rating = rating_np[t_i][2]\n",
    "\n",
    "        if user not in user_history_dict:\n",
    "            user_history_dict[user] = [item]\n",
    "        user_history_dict[user].append(item)\n",
    "\n",
    "    return user_history_dict\n",
    "\n",
    "def load_kg(d):\n",
    "    print('reading KG file ...')\n",
    "    # reading kg file\n",
    "    kg_np = np.load('../data/' + d + '/kg_final.npy')\n",
    "\n",
    "    n_entity = len(set(kg_np[:, 0]) | set(kg_np[:, 2]))\n",
    "    n_relation = len(set(kg_np[:, 1]))\n",
    "    print('e %d, r %d' % (n_entity, n_relation))\n",
    "    kg = collections.defaultdict(list)\n",
    "    for h, r, t in kg_np:\n",
    "        kg[h].append((t, r))\n",
    "\n",
    "    return kg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading rating file ...\n",
      "u 6969, i 9854\n",
      "reading KG file ...\n",
      "e 113487, r 39\n"
     ]
    }
   ],
   "source": [
    "d = 'amazon-book'\n",
    "user_history_dict = load_rating(d)\n",
    "kg = load_kg(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing ripple set ...\n",
      "cost: 63.265\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "h = 2\n",
    "m = 32\n",
    "c = 8\n",
    "r, entity_interaction_dict = get_ripple_set(h, m, c)\n",
    "print('cost: %.3f' % (time.time() - t_start))\n",
    "\n",
    "\n",
    "user_total = []\n",
    "user_total_interact = []\n",
    "for e_i in entity_interaction_dict.values():\n",
    "    user_total_interact += e_i\n",
    "t_start = time.time()\n",
    "get_total_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "# avg_entity_interaction_tmp = len(list(set(user_total))) / len(total_entity)\n",
    "for u, its in user_history_dict.items():\n",
    "    user_total += its\n",
    "    buffer = its.copy()\n",
    "    for _ in range(h):\n",
    "        _next = []\n",
    "        for i in set(buffer):\n",
    "            tails = [t for t, _ in kg[i]]\n",
    "            user_total += tails\n",
    "            _next += tails\n",
    "        buffer = _next.copy()\n",
    "\n",
    "print(len(list(set(user_total_interact))))\n",
    "print(len(list(set(user_total))))\n",
    "print('cost: %.3f' % (time.time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ripple_set(h=2, m=32, c=12):\n",
    "    print('constructing ripple set ...')\n",
    "\n",
    "    # user -> [(hop_0_heads, hop_0_relations, hop_0_tails), (hop_1_heads, hop_1_relations, hop_1_tails), ...]\n",
    "    ripple_set = collections.defaultdict(list)\n",
    "    entity_interaction_dict = collections.defaultdict(list)\n",
    "    total_entity = []\n",
    "    with mp.Pool(processes=min(mp.cpu_count(), c)) as pool:\n",
    "        job = partial(_get_ripple_set, n_hop=h, n_memory=m)\n",
    "#         ripple_set = {ret[0]: np.array(ret[1], dtype=np.int32) for ret in pool.starmap(job, user_history_dict.items())}\n",
    "        for u, u_r_set, u_interaction_list in pool.starmap(job, user_history_dict.items()):\n",
    "            ripple_set[u] = np.array(u_r_set, dtype=np.int32)\n",
    "            entity_interaction_dict[u] = u_interaction_list\n",
    "    return ripple_set, entity_interaction_dict\n",
    "\n",
    "def _get_ripple_set(user, history, n_hop=2, n_memory=32):\n",
    "    ret = []\n",
    "    entity_interaction_list = []\n",
    "    total = history.copy()\n",
    "    for h in range(n_hop):\n",
    "        memories_h = []\n",
    "        memories_r = []\n",
    "        memories_t = []\n",
    "\n",
    "        if h == 0:\n",
    "            tails_of_last_hop = history\n",
    "        else:\n",
    "            tails_of_last_hop = ret[-1][2]\n",
    "\n",
    "        for entity in tails_of_last_hop:\n",
    "#             for tail_and_relation in kg[entity]:\n",
    "            random.seed(time.time())\n",
    "            for tail_and_relation in random.sample(kg[entity], min(len(kg[entity]), 16)):\n",
    "                memories_h.append(entity)\n",
    "                memories_r.append(tail_and_relation[1])\n",
    "                memories_t.append(tail_and_relation[0])\n",
    "        # if the current ripple set of the given user is empty, we simply copy the ripple set of the last hop here\n",
    "        # this won't happen for h = 0, because only the items that appear in the KG have been selected\n",
    "        # this only happens on 154 users in Book-Crossing dataset (since both BX dataset and the KG are sparse)\n",
    "        if len(memories_h) == 0:\n",
    "            ret.append(ret[-1])\n",
    "        else:\n",
    "            indices = np.random.choice(len(memories_h), size=n_memory, replace=len(memories_h) < n_memory)\n",
    "            memories_h = [memories_h[i] for i in indices]\n",
    "            memories_r = [memories_r[i] for i in indices]\n",
    "            memories_t = [memories_t[i] for i in indices]\n",
    "            entity_interaction_list += memories_h + memories_t\n",
    "            ret.append([memories_h, memories_r, memories_t])\n",
    "    return [user, ret, list(set(entity_interaction_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
